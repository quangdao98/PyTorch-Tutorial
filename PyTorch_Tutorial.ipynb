{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Note:\n",
    "To run this notebook as slides, download and install RISE from this link: https://github.com/damianavila/RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this presentation, I will introduce PyTorch - a new deep learning framework that is picking up use thanks to its fast computation and convenience.\n",
    "\n",
    "Then, I will give a PyTorch implementation of text classification using CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a25a176e-93c1-4a39-b435-7992399ed23b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors\n",
    "\n",
    "A Tensor is a n-dimensional array, i.e: a 1D Tensor is an array, while a 2D Tensor is a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tensors are the fundamental building blocks of PyTorch. PyTorch provides tensors of different types and also support accelerated performance on tensors using GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b7c1382-4c07-4e82-9cde-6d7024f843e8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initializing Tensors\n",
    "As a beginning example, we will see how we can create a PyTorch Tensor from a simple Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "04b76158-8b46-4e45-a061-9c9b1ac4893e"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2  3\n",
      " 2  3  4\n",
      "[torch.LongTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.LongTensor([[1,2,3],[2,3,4]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9722f20a-f5d6-49a1-9a44-a53c4897cb7c"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "PyTorch also integrates well with `numpy`, so we can pass in a numpy `ndarray` to create a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "a5241b08-54ab-45da-abab-15cc37119e6a"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.ndarray([2,3])\n",
    "x = torch.Tensor(y)\n",
    "print(x)\n",
    "z = x.numpy()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we have GPU available, we can pass our Tensor to be computed by the GPU using PyTorch's command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The library also offers many methods of its own to create new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "3c8f6268-22dd-4101-97db-2f7316fc215d"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5994  1.3048  0.9864 -0.7510 -0.2672\n",
      "-0.4711  1.3678 -0.2598  1.2567 -0.4186\n",
      " 0.7078 -0.6801 -0.0614 -1.1212 -0.3434\n",
      " 0.7537  0.0479 -0.1500  0.1030  0.1707\n",
      "[torch.FloatTensor of size 4x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4,5) # Numbers are taken from a normal distribution with mean=0, var=1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "015e5414-2372-472d-816d-f37fd7f4fd82"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(3,4)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f57a6e07-213d-422a-a70a-6f421b3b48ad"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operations on Tensors\n",
    "As a framework that puts tensors first, Torch offers a wide range of tensor operations.\n",
    "\n",
    "Most of Python's basic operations on n-dimensional arrays also work with `torch.Tensor`; there are also operations in PyTorch's library that does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "4241be69-0ea3-40c6-b1d4-f738ec309df4"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3  5  7\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,3]])\n",
    "y = torch.Tensor([[2,3,4]])\n",
    "z = x + y\n",
    "z = torch.add(x,y) # This is equivalent!\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5a1c944e-bc83-4145-a8e6-21da07a96817"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition to common math expressions like addition and multiplication, PyTorch also offers a wide variety of other operations on Tensors that make them easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "As an example, here are three operations that I usually use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The concatenation operator can concatenate multiple vectors in a specified dimension.\n",
    "\n",
    "If the dimension is not defined, it is assumed to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "1abeb790-5540-486b-a3de-159997c9a2bc"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2  3\n",
      " 2  3  4\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      " 1  2  3  2  3  4\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,3]])\n",
    "y = torch.Tensor([[2,3,4]])\n",
    "z = torch.cat((x,y)) # Concatenate along the 0-dimension, a.k.a: the row\n",
    "print(z)\n",
    "t = torch.cat((x,y), dim=1)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `view` operation offers the ability to reshape our Tensor at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "3f4a51f6-4b81-4058-bdc6-5c7c0110e0c6"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.1394 -1.0874 -0.2238  0.1953\n",
      " -0.0727  0.5987 -0.0808  0.2204\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0464  1.4881 -0.8613 -0.6313\n",
      "  0.9307 -0.7568  0.0781 -0.2241\n",
      "[torch.FloatTensor of size 2x2x4]\n",
      "\n",
      "\n",
      " 0.1394 -1.0874 -0.2238  0.1953\n",
      "-0.0727  0.5987 -0.0808  0.2204\n",
      "-0.0464  1.4881 -0.8613 -0.6313\n",
      " 0.9307 -0.7568  0.0781 -0.2241\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2,4)\n",
    "y = x.view(4,4)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It can also infer the missing dimension in the resulting Tensor if we pass in -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "1d004ba4-a316-44a9-a9dc-ad508d9e9270"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1394 -1.0874 -0.2238  0.1953 -0.0727  0.5987 -0.0808  0.2204\n",
      "-0.0464  1.4881 -0.8613 -0.6313  0.9307 -0.7568  0.0781 -0.2241\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x has size 2x2x4\n",
    "z = x.view(-1,8)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the `squeeze` operator will remove all \"trivial\" dimensions, i.e: dimensions that has size 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbpresent": {
     "id": "db67ce2a-c48b-4f94-b00c-d6991e44826c"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -1.1949  0.3846 -1.0274\n",
      "  1.1751  1.3324 -0.2114\n",
      "[torch.FloatTensor of size 1x2x3]\n",
      "\n",
      "\n",
      "-1.1949  0.3846 -1.0274\n",
      " 1.1751  1.3324 -0.2114\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,2,3) # x size 1x2x3\n",
    "print(x)\n",
    "y = x.squeeze() # y will have size 2x3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "By contrast, the `unsqueeze` operator adds a trivial dimension to our Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      " -1.1949  0.3846 -1.0274\n",
      "  1.1751  1.3324 -0.2114\n",
      "[torch.FloatTensor of size 1x1x2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remember that x has size 1x2x3\n",
    "z = x.unsqueeze(0) # z will have size 1x1x2x3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "These operators can be used when we need to reshape our Tensor to have the right size for certain neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b137086f-d375-46a0-bf96-9b8526879036"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autograd mechanism and automatic differentiation\n",
    "One of the neat things about PyTorch is that the framework supports automatic differentiation; just define how a quantity is computed and PyTorch will immediately calculate the gradients.\n",
    "\n",
    "This is done via the `Variable` class in `torch.autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbpresent": {
     "id": "a6866c28-f4dd-492f-930d-f5b47a32d6fc"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  4  5\n",
      " 3  4  2\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.Tensor([[1,2,3],[2,3,1]]))\n",
    "y = Variable(torch.Tensor([[2,2,2],[1,1,1]]))\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, notice the fact that compared to `z.data`, which is a Tensor, `z` has an extra line saying `Variable containing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  4  5\n",
      " 3  4  2\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      " 3  4  5\n",
      " 3  4  2\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "print(z.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`z` also knows how it was created. This is useful for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd._functions.basic_ops.Add object at 0x7fbe08e07e48>\n"
     ]
    }
   ],
   "source": [
    "print(z.creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to Tensors, we can also move our Variables to GPU in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    z = z.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far, we haven't computed any gradients yet. We can ask PyTorch to calculate gradients for each `Variable` automatically via the `backward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbpresent": {
     "id": "f23af919-09d8-4970-8bf9-9b47fecc2061"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# backward() only works on scalar, so we need to create one\n",
    "s = z.sum()\n",
    "print(s)\n",
    "# Since we haven't called backward() yet, no gradient is found for x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbpresent": {
     "id": "928c9df1-c6cc-492b-8f5c-2247f5fdb639"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1  1\n",
      " 1  1  1\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we call backward()\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "PyTorch accumulates the gradient after each call of `backward()`. Thus, if we call `s.backward()` again, the new gradient will be added to the existing `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  2  2\n",
      " 2  2  2\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 3  3  3\n",
      " 3  3  3\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x.grad will now double in value\n",
    "s.backward()\n",
    "print(x.grad)\n",
    "# x.grad will now triple\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To clear gradients, we need to set them to 0. Here's how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Later, we will see another way to zero the gradients in our `Variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3285f1ab-bb57-4a4d-b204-2a89ce4286b7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network\n",
    "PyTorch offers many common building blocks for a deep learning architecture such as fully-connected layer, convolution, pooling, embedding, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "We can create our own deep learning model using such building blocks in a very flexible and convenient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All we need to do is to create our model class that extends `torch.nn.Module` and implements the `__init__()` and `forward()` method. The first method is invoked to read in any parameters and define any layers that we need, and the latter to specify how the forward part of training works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "30807a6d-f7be-4618-88b7-c92c9d50c386"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# nn.functional contains many common functions in deep learning, i.e: relu, sigmoid,...\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A simple feed forward network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, init_dim, hid_dim, out_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(init_dim, hid_dim)\n",
    "        self.layer2 = nn.Linear(hid_dim, out_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        layer1_outputs = F.relu(self.layer1(inputs))\n",
    "        layer2_outputs = F.log_softmax(self.layer2(layer1_outputs))\n",
    "        \n",
    "        return layer2_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1daec3e6-9b38-44f0-bd42-d0d8eed47d6b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a network\n",
    "\n",
    "After we have defined our model, time to do some training! In addition to our model, we need to specify how to train it: what kind of loss function to use, and similarly choosing the optimization method, i.e: stochastic gradient descents or more advanced optimizers such as `Adam` or `RMSprop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we shall see below, PyTorch's automatic differentiation makes our training procedure much easier: the backward pass of training is as simple as calling `backward()` on our loss function and `step()` on our optimizer to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbpresent": {
     "id": "3131b690-62c2-44c4-8ac4-73825a374125"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.2490\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2464\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2451\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2438\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2425\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2411\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2398\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2385\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2372\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = NeuralNet(20,10,5)\n",
    "inputs, labels = Variable(torch.randn(10,20)), Variable(torch.LongTensor(10).zero_())\n",
    "\n",
    "if torch.cuda.is_available(): # GPU support\n",
    "    model.cuda() # Will move all model parameters to GPU\n",
    "    inputs, labels = inputs.cuda(), labels.cuda() # Will move training data to GPU\n",
    "    \n",
    "loss_function = nn.NLLLoss() # Negative log likelihood loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3) # Stochastic gradient descent\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() # Zero the gradients of all model parameters\n",
    "    # model.zero_grad() is equivalent\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we are done with the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you don't understand everything yet, don't worry! You can always go to [PyTorch's documentation](http://pytorch.org/docs/master/) or check out one of PyTorch's [many tutorials](http://pytorch.org/tutorials/) on its official site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the following section, I will present an application of CNN to text classification, implemented in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9ff14198-9bde-4bb3-a1d8-eb136773077d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "99aa8385-fb74-44a5-8442-11e7d23db340"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Convolutional Neural Network (CNN) has long been known as a good feature extractor for images, i.e: success of CNN in classifying ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Given the fact that convolutional layers can extract local information from the data, as well as being translation invariant, we can also use it to achieve good results on sentence classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "In Yoon Kim's 2014 paper, Convolutional Neural Networks for Sentence Classification, he successfully apply CNN to achieve state-of-the-art results in many tasks related to sentence classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec2aba59-3ca5-4cf6-8835-93989b09e7e1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Architecture\n",
    "<img src=\"./CNN_architecture.png\" alt=\"CNN_model\" style=\"width: 550px; display:block; margin:auto; \"/>\n",
    "\n",
    "<sup><sub>__Source:__ Zhang, Y., & Wallace, B. (2015). _A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification._</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "420e77ce-6432-4f0c-9a0e-8579b4d9e440"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7b35033d-f5fa-4762-b81a-6b8b8634484d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"./CNN_acc1.png\" alt=\"Accuracy with CNN model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "150b1e85-10f1-4bcc-bda5-5d66533ce5d8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Future Directions\n",
    "While the result presented above is promising, this is far from achieving state-of-the-art results and would require more analysis to reach that level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here are some of the steps I could take to achieve further results:\n",
    "* Implement more sophisticated architectures (LSTM)\n",
    "\n",
    "* Find larger datasets to train\n",
    "\n",
    "* Integrate into the company's codebase"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbpresent": {
   "slides": {
    "09c61a02-8161-49a8-b442-a1f7469ca7de": {
     "id": "09c61a02-8161-49a8-b442-a1f7469ca7de",
     "prev": "112927fb-6187-4632-839f-fc9a7e15b3ac",
     "regions": {
      "7ef12969-61e8-4c51-8fa1-ea56f42cebc7": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "015e5414-2372-472d-816d-f37fd7f4fd82",
        "part": "whole"
       },
       "id": "7ef12969-61e8-4c51-8fa1-ea56f42cebc7"
      },
      "8f7bb779-c411-4d8e-8ef0-9777a3ed4b20": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "3c8f6268-22dd-4101-97db-2f7316fc215d",
        "part": "whole"
       },
       "id": "8f7bb779-c411-4d8e-8ef0-9777a3ed4b20"
      },
      "e2a93d34-cf0f-44a6-9e96-05511ea001c4": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cfcd5d0f-6ce9-41b6-8775-e180b04f4ea3",
        "part": "whole"
       },
       "id": "e2a93d34-cf0f-44a6-9e96-05511ea001c4"
      }
     }
    },
    "112927fb-6187-4632-839f-fc9a7e15b3ac": {
     "id": "112927fb-6187-4632-839f-fc9a7e15b3ac",
     "prev": "a84b2e93-b1cc-471b-8a53-3f3a083c2f0c",
     "regions": {
      "857be000-d3ad-4e77-ba42-0a6939a900df": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9722f20a-f5d6-49a1-9a44-a53c4897cb7c",
        "part": "whole"
       },
       "id": "857be000-d3ad-4e77-ba42-0a6939a900df"
      },
      "dd6e50b8-4111-4caf-b4de-7a9a59f6c37b": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "a5241b08-54ab-45da-abab-15cc37119e6a",
        "part": "whole"
       },
       "id": "dd6e50b8-4111-4caf-b4de-7a9a59f6c37b"
      }
     }
    },
    "125c90af-0ff6-4c30-ad99-d398836f4a9e": {
     "id": "125c90af-0ff6-4c30-ad99-d398836f4a9e",
     "prev": "ca6eb579-416a-445d-b4bf-5d386bb9f66e",
     "regions": {
      "9a6708ff-8109-4644-8a3b-4816d6fccb3a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9ff14198-9bde-4bb3-a1d8-eb136773077d",
        "part": "whole"
       },
       "id": "9a6708ff-8109-4644-8a3b-4816d6fccb3a"
      }
     }
    },
    "1507e2d2-51e3-4b15-8178-aa72996a3790": {
     "id": "1507e2d2-51e3-4b15-8178-aa72996a3790",
     "prev": "cfb2cf9d-f115-46c4-805a-4e61b169adb0",
     "regions": {
      "058cbb50-922e-47ac-975c-39d698031424": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "3f4a51f6-4b81-4058-bdc6-5c7c0110e0c6",
        "part": "whole"
       },
       "id": "058cbb50-922e-47ac-975c-39d698031424"
      },
      "455a00ad-f5c5-45b6-8212-90fab0bc50de": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5a1c944e-bc83-4145-a8e6-21da07a96817",
        "part": "whole"
       },
       "id": "455a00ad-f5c5-45b6-8212-90fab0bc50de"
      },
      "45b5a7a0-efd7-4ef5-a026-c3fee9f45667": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "1abeb790-5540-486b-a3de-159997c9a2bc",
        "part": "whole"
       },
       "id": "45b5a7a0-efd7-4ef5-a026-c3fee9f45667"
      },
      "5db09b65-da28-4f32-acd6-4db4909868a8": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "1d004ba4-a316-44a9-a9dc-ad508d9e9270",
        "part": "whole"
       },
       "id": "5db09b65-da28-4f32-acd6-4db4909868a8"
      },
      "f7044e17-338a-4bec-95d9-f408d3473235": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "db67ce2a-c48b-4f94-b00c-d6991e44826c",
        "part": "whole"
       },
       "id": "f7044e17-338a-4bec-95d9-f408d3473235"
      }
     }
    },
    "1c11937e-b11a-4114-859d-c8779214776d": {
     "id": "1c11937e-b11a-4114-859d-c8779214776d",
     "layout": "grid",
     "prev": "db65798b-05ec-49c9-aab8-9674b415b0bd",
     "regions": {
      "8bc56d8c-fb36-496c-a62e-ea495c41af26": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "a25a176e-93c1-4a39-b435-7992399ed23b",
        "part": "whole"
       },
       "id": "8bc56d8c-fb36-496c-a62e-ea495c41af26",
       "treemap:weight": 0.8
      }
     },
     "theme": "d997bc00-5626-460e-87db-2f5c6d8e78f3"
    },
    "7938ed7f-4101-430c-9444-d7dc6761737c": {
     "id": "7938ed7f-4101-430c-9444-d7dc6761737c",
     "prev": "b73570a1-37aa-450a-aaab-f3cdb2ebd25a",
     "regions": {
      "3d2ec878-be17-4128-875c-1fc2485b18c9": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "30807a6d-f7be-4618-88b7-c92c9d50c386",
        "part": "whole"
       },
       "id": "3d2ec878-be17-4128-875c-1fc2485b18c9"
      },
      "78bde77f-3a73-4c69-813e-c6f11e81a476": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3285f1ab-bb57-4a4d-b204-2a89ce4286b7",
        "part": "whole"
       },
       "id": "78bde77f-3a73-4c69-813e-c6f11e81a476"
      }
     }
    },
    "a84b2e93-b1cc-471b-8a53-3f3a083c2f0c": {
     "id": "a84b2e93-b1cc-471b-8a53-3f3a083c2f0c",
     "layout": "manual",
     "prev": "1c11937e-b11a-4114-859d-c8779214776d",
     "regions": {
      "231994d0-969d-4128-8083-25d5cf66906c": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.9955819858840677,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "04b76158-8b46-4e45-a061-9c9b1ac4893e",
        "part": "whole"
       },
       "id": "231994d0-969d-4128-8083-25d5cf66906c"
      },
      "757d12a3-c95e-4bba-ab83-261b9d95cff4": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.994621555678866,
        "x": 0.005378444321134002,
        "y": 0
       },
       "content": {
        "cell": "2b7c1382-4c07-4e82-9cde-6d7024f843e8",
        "part": "whole"
       },
       "id": "757d12a3-c95e-4bba-ab83-261b9d95cff4"
      }
     },
     "theme": null
    },
    "b73570a1-37aa-450a-aaab-f3cdb2ebd25a": {
     "id": "b73570a1-37aa-450a-aaab-f3cdb2ebd25a",
     "prev": "1507e2d2-51e3-4b15-8178-aa72996a3790",
     "regions": {
      "0ba5b5b4-fc21-4299-b6a0-2e2c7e052aa8": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "a6866c28-f4dd-492f-930d-f5b47a32d6fc",
        "part": "whole"
       },
       "id": "0ba5b5b4-fc21-4299-b6a0-2e2c7e052aa8"
      },
      "196205b4-221b-4008-b9b2-297bc7635aa8": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "f23af919-09d8-4970-8bf9-9b47fecc2061",
        "part": "whole"
       },
       "id": "196205b4-221b-4008-b9b2-297bc7635aa8"
      },
      "2c49a9e0-89fd-4d51-9601-176b995fed80": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b137086f-d375-46a0-bf96-9b8526879036",
        "part": "whole"
       },
       "id": "2c49a9e0-89fd-4d51-9601-176b995fed80"
      },
      "8f9fb9f7-2f8f-451f-bfc7-a35439df86b5": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "928c9df1-c6cc-492b-8f5c-2247f5fdb639",
        "part": "whole"
       },
       "id": "8f9fb9f7-2f8f-451f-bfc7-a35439df86b5"
      }
     }
    },
    "bf68baee-49b4-46ea-a302-fbdf558ffe17": {
     "id": "bf68baee-49b4-46ea-a302-fbdf558ffe17",
     "prev": "125c90af-0ff6-4c30-ad99-d398836f4a9e",
     "regions": {
      "37e00966-dbb8-4c60-9a7f-ac49438a587c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "99aa8385-fb74-44a5-8442-11e7d23db340",
        "part": "whole"
       },
       "id": "37e00966-dbb8-4c60-9a7f-ac49438a587c"
      }
     }
    },
    "c773a571-82b0-478f-ae57-cc554fb3fea4": {
     "id": "c773a571-82b0-478f-ae57-cc554fb3fea4",
     "prev": "f59b0c99-4b49-48a7-ac69-5ae94c30e689",
     "regions": {
      "ead4b744-818f-426f-a29f-c97808b46da2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7b35033d-f5fa-4762-b81a-6b8b8634484d",
        "part": "whole"
       },
       "id": "ead4b744-818f-426f-a29f-c97808b46da2"
      }
     }
    },
    "ca6eb579-416a-445d-b4bf-5d386bb9f66e": {
     "id": "ca6eb579-416a-445d-b4bf-5d386bb9f66e",
     "prev": "7938ed7f-4101-430c-9444-d7dc6761737c",
     "regions": {
      "baaed5f0-22ab-4ee7-958a-cd42e6d29280": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "3131b690-62c2-44c4-8ac4-73825a374125",
        "part": "whole"
       },
       "id": "baaed5f0-22ab-4ee7-958a-cd42e6d29280"
      },
      "d4771ea4-bcb9-40e0-8746-771f81c96abe": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1daec3e6-9b38-44f0-bd42-d0d8eed47d6b",
        "part": "whole"
       },
       "id": "d4771ea4-bcb9-40e0-8746-771f81c96abe"
      }
     }
    },
    "cfb2cf9d-f115-46c4-805a-4e61b169adb0": {
     "id": "cfb2cf9d-f115-46c4-805a-4e61b169adb0",
     "prev": "09c61a02-8161-49a8-b442-a1f7469ca7de",
     "regions": {
      "68290fb4-3335-40fa-bcff-d2bd3b0b8777": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "4241be69-0ea3-40c6-b1d4-f738ec309df4",
        "part": "whole"
       },
       "id": "68290fb4-3335-40fa-bcff-d2bd3b0b8777"
      },
      "6de06921-5902-4b7d-9bad-42d38afd0a3b": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "272e566b-b7f6-4fda-a43e-a30460ce99eb",
        "part": "whole"
       },
       "id": "6de06921-5902-4b7d-9bad-42d38afd0a3b"
      },
      "f9921226-37c8-45a2-81f5-7fd72f1431d5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f57a6e07-213d-422a-a70a-6f421b3b48ad",
        "part": "whole"
       },
       "id": "f9921226-37c8-45a2-81f5-7fd72f1431d5"
      }
     }
    },
    "db65798b-05ec-49c9-aab8-9674b415b0bd": {
     "id": "db65798b-05ec-49c9-aab8-9674b415b0bd",
     "layout": "manual",
     "prev": null,
     "regions": {
      "91b1132a-6d0e-4ea7-9fa5-d9e769292a4d": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1.8500000000000003,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "e26a563d-b1a4-4ce1-9df6-2a62efaedf43",
        "part": "whole"
       },
       "height": 0.5,
       "id": "91b1132a-6d0e-4ea7-9fa5-d9e769292a4d",
       "pad": 0.5,
       "treemap:weight": 1,
       "width": 0.7
      }
     },
     "theme": "d997bc00-5626-460e-87db-2f5c6d8e78f3"
    },
    "f59b0c99-4b49-48a7-ac69-5ae94c30e689": {
     "id": "f59b0c99-4b49-48a7-ac69-5ae94c30e689",
     "prev": "bf68baee-49b4-46ea-a302-fbdf558ffe17",
     "regions": {
      "30162b59-8393-47c5-be4a-a8fd99ba2b60": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "420e77ce-6432-4f0c-9a0e-8579b4d9e440",
        "part": "whole"
       },
       "id": "30162b59-8393-47c5-be4a-a8fd99ba2b60"
      },
      "887addb7-586e-46cc-830c-ed434884d1c5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec2aba59-3ca5-4cf6-8835-93989b09e7e1",
        "part": "whole"
       },
       "id": "887addb7-586e-46cc-830c-ed434884d1c5"
      }
     }
    },
    "f7c415e2-4eff-4544-8275-8f0514a460cd": {
     "id": "f7c415e2-4eff-4544-8275-8f0514a460cd",
     "prev": "c773a571-82b0-478f-ae57-cc554fb3fea4",
     "regions": {
      "bf3ed1ed-9137-4d32-b01b-e369f6f95ae8": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "d2b4e785-7985-463b-9948-401776270d83",
        "part": "whole"
       },
       "id": "bf3ed1ed-9137-4d32-b01b-e369f6f95ae8"
      },
      "c5adefcb-61ab-436d-a07d-73c0abcfb859": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "150b1e85-10f1-4bcc-bda5-5d66533ce5d8",
        "part": "whole"
       },
       "id": "c5adefcb-61ab-436d-a07d-73c0abcfb859"
      }
     }
    }
   },
   "themes": {
    "default": "4b10ac3c-0a06-44e7-9670-52b6eaba9f66",
    "theme": {
     "4b10ac3c-0a06-44e7-9670-52b6eaba9f66": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "4b10ac3c-0a06-44e7-9670-52b6eaba9f66",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         253,
         246,
         227
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         88,
         110,
         117
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         38,
         139,
         210
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         101,
         123,
         131
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
